{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2271dcbb-3c67-42ae-8004-6976860afd06",
   "metadata": {},
   "source": [
    "# Notebook for the paper \"Lip Abnormality Detection for Patients with Repaired Cleft Lip and Palate: A Lip Normalization Approach\" \n",
    "\n",
    "## Authors: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c03192-ac80-43e7-af09-1e37e108eb41",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6bfaf121-8851-4e88-be57-9b0fafb0706c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utility_functions.utils as uf\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from third_party.pyto_tool import EarlyStopping\n",
    "import random\n",
    "import os\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b99dba5-0b26-48fa-8618-5cfc8e464cb9",
   "metadata": {},
   "source": [
    "### Model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "038e3025-0d3d-4774-a236-4fa0893e43e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "seed = 42\n",
    "seed_everything(seed)\n",
    "batch_size = 16\n",
    "threshold = 57\n",
    "learning_rate = 0.00001\n",
    "patience = 3\n",
    "n_epochs = 50\n",
    "dir_path = './weights/siamesecnn_checkpoint.pt'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b081d0ed-0b46-48d1-a4f8-b02e969fb278",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a5f610f-18ee-436c-b58f-d15c0aba170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Control adult image subsets\n",
    "train_cr = pd.read_csv('./data/split_data/train_cr.csv')\n",
    "val_cr   = pd.read_csv('./data/split_data/val_cr.csv')\n",
    "test_cr  = pd.read_csv('./data/split_data/test_cr.csv')\n",
    "\n",
    "# Augmented adult image subsets\n",
    "train_au = pd.read_csv('./data/split_data/train_au.csv')\n",
    "val_au   = pd.read_csv('./data/split_data/val_au.csv')\n",
    "test_au  = pd.read_csv('./data/split_data/test_au.csv')\n",
    "\n",
    "# Control children image subsets\n",
    "train_Ccr = pd.read_csv('./data/split_data/train_Ccr.csv')\n",
    "val_Ccr   = pd.read_csv('./data/split_data/val_Ccr.csv')\n",
    "test_Ccr  = pd.read_csv('./data/split_data/test_Ccr.csv')\n",
    "\n",
    "# Augmented children image subsets\n",
    "train_Cau = pd.read_csv('./data/split_data/train_Cau.csv')\n",
    "val_Cau   = pd.read_csv('./data/split_data/val_Cau.csv')\n",
    "test_Cau  = pd.read_csv('./data/split_data/test_Cau.csv')\n",
    "\n",
    "# CLP images \n",
    "clp = './data/CLP/'\n",
    "clp_m = './data/CLP_norm/'\n",
    "common_files_clp = uf.common_files(clp, clp_m, '.npy')\n",
    "clp_m_path = [clp_m+path.split('.')[-2]+'.png' for path in common_files_clp]\n",
    "clp_path = [clp+path.split('.')[-2]+'.png' for path in common_files_clp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "32d48d1f-26ee-4fcd-893d-2858a74d7188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 2299 \n",
      "Val samples: 577 \n",
      "Test samples: 511 \n",
      "Test adult samples: 186 \n",
      "Test children samples: 69 \n",
      "Test CLP samples: 395\n"
     ]
    }
   ],
   "source": [
    "# Concat subsets of adults, children for train, val, test\n",
    "train_df = pd.concat([train_cr, train_au, train_Ccr, train_Cau], ignore_index=True)\n",
    "val_df = pd.concat([val_cr, val_au, val_Ccr, val_Cau], ignore_index=True)\n",
    "test_df = pd.concat([test_cr, test_au, test_Ccr, test_Cau], ignore_index=True)\n",
    "\n",
    "test_cr = test_cr.reset_index(drop=True)\n",
    "test_Ccr = test_Ccr.reset_index(drop=True)\n",
    "\n",
    "test_clp = pd.DataFrame(data={'img': clp_path, 'img_n': clp_m_path, 'label':[0] * len(clp_m_path)})\n",
    "\n",
    "print('Train samples:', len(train_df), '\\nVal samples:',len(val_df), \n",
    "      '\\nTest samples:',len(test_df),  '\\nTest adult samples:',len(test_cr), \n",
    "      '\\nTest children samples:',len(test_Ccr), '\\nTest CLP samples:',len(test_clp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c0deeee-523f-469f-83c5-407686ab7609",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipsDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, data_dir, transform=None, dataset_name=None):\n",
    "        self.transform = transform\n",
    "        self.data = data_dir\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "            \n",
    "        # Get images before and after normalization   \n",
    "        path = self.data.loc[idx, 'img']\n",
    "        path_m = self.data.loc[idx, 'img_n']\n",
    "        img = cv2.imread(path)\n",
    "        img_m = cv2.imread(path_m)\n",
    "        \n",
    "        # Get label\n",
    "        label = self.data.loc[idx, 'label']\n",
    "        \n",
    "        # Get landmarks        \n",
    "        if (self.dataset_name is 'CLP2'):\n",
    "            lks = np.load(path_m[:-3]+'npy')\n",
    "        else:\n",
    "            lks = np.load(path[:-3]+'npy')\n",
    "            \n",
    "\n",
    "        # Crop images based on the lip landmarks    \n",
    "        img, _ = uf.crop_img_lks_lips(img, lks, 0.2)      \n",
    "        img_m, _ = uf.crop_img_lks_lips(img_m, lks, 0.2)\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "            img_m = self.transform(img_m)\n",
    "            \n",
    "        return img, img_m, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32eabae1-8048-4834-ad85-54241dce18d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = torchvision.transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224))   , \n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4626cb5b-8a61-4c69-8046-a71a72244d2d",
   "metadata": {},
   "source": [
    "#### Dataloaders definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08058057-2162-4be9-b8ab-70741759e705",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = LipsDataset(train_df, transform = preprocess, dataset_name=None)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_dataset = LipsDataset(val_df, transform = preprocess, dataset_name=None)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = LipsDataset(test_df, transform = preprocess, dataset_name=None)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_clp_dataset = LipsDataset(test_clp, transform = preprocess, dataset_name='CLP2')\n",
    "test_clp_dataloader = DataLoader(test_clp_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_child_dataset = LipsDataset(test_Ccr, transform = preprocess, dataset_name=None)\n",
    "test_child_dataloader = DataLoader(test_child_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "test_adult_dataset = LipsDataset(test_cr, transform = preprocess,  dataset_name=None)\n",
    "test_adult_dataloader = DataLoader(test_adult_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea30c8d-8398-4ada-b9aa-d05c4bcdb4c8",
   "metadata": {},
   "source": [
    "### Siamese CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e3a07b6a-9691-46dd-86df-d66abae599c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.model = torch.hub.load('pytorch/vision:v0.10.0', 'mobilenet_v2', pretrained=True)#weights='IMAGENET1K_V1') \n",
    "        self.model.classifier[-1] = torch.nn.Identity()\n",
    "        self.clp1 = torch.nn.Linear(1280*2, 128)\n",
    "        self.drop = torch.nn.Dropout(p = 0.2) \n",
    "        self.clp2 = torch.nn.Linear(128, 2)\n",
    "        \n",
    "    def _sample_forward_lip(self, x):\n",
    "        out = self.model(x)\n",
    "        return out\n",
    "     \n",
    "    def forward(self, lip, lip_m):\n",
    "        \n",
    "        out_emb = self._sample_forward_lip(lip)\n",
    "        out_emb_m = self._sample_forward_lip(lip_m)\n",
    "\n",
    "        out_concat = torch.cat((out_emb, out_emb_m), 1)\n",
    "        \n",
    "        out_concat = F.relu(self.clp1(out_concat))\n",
    "        out_concat = self.drop(out_concat)\n",
    "        out_concat = self.clp2(out_concat)\n",
    "\n",
    "        return out_concat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1e0fa8-ea68-4ac8-b577-a532522e1db6",
   "metadata": {},
   "source": [
    "#### Model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dcaed4ef-9594-4431-a302-d62e8d8fb34d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/kgrosero/.cache/torch/hub/pytorch_vision_v0.10.0\n",
      "/home/kgrosero/miniconda3/envs/syncnet/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and may be removed in the future, \"\n",
      "/home/kgrosero/miniconda3/envs/syncnet/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V2_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V2_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "model = SiameseCNN().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "criterion_class = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Number of layers to be finetuned, defined by threshold\n",
    "for i, param in enumerate(model.model.features.parameters()):\n",
    "    if i < threshold:\n",
    "        param.requires_grad = False\n",
    "    elif i >= threshold:\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a3b175-f659-4ea2-9a27-0a108f93c3c9",
   "metadata": {},
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a725009-4339-48c0-8ae9-7a942323c45f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch: 1"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:26<00:00,  5.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.5706829056111353\n",
      "Train loss:  0.6774761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.729636048526863\n",
      "Valid loss:  0.6478124\n",
      "Validation loss decreased (inf --> 0.647812).  Saving model ...\n",
      "Training epoch: 2"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:24<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.729447585906916\n",
      "Train loss:  0.6104839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.8336221837088388\n",
      "Valid loss:  0.5278925\n",
      "Validation loss decreased (0.647812 --> 0.527892).  Saving model ...\n",
      "Training epoch: 3"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.8534145280556764\n",
      "Train loss:  0.44189015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9150779896013865\n",
      "Valid loss:  0.30387676\n",
      "Validation loss decreased (0.527892 --> 0.303877).  Saving model ...\n",
      "Training epoch: 4"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9134406263592867\n",
      "Train loss:  0.27935374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9376083188908145\n",
      "Valid loss:  0.20694612\n",
      "Validation loss decreased (0.303877 --> 0.206946).  Saving model ...\n",
      "Training epoch: 5"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  4.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9299695519791213\n",
      "Train loss:  0.2130886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9601386481802426\n",
      "Valid loss:  0.14452404\n",
      "Validation loss decreased (0.206946 --> 0.144524).  Saving model ...\n",
      "Training epoch: 6"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9473684210526315\n",
      "Train loss:  0.15977444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9688041594454073\n",
      "Valid loss:  0.12631856\n",
      "Validation loss decreased (0.144524 --> 0.126319).  Saving model ...\n",
      "Training epoch: 7"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:25<00:00,  5.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9617224880382775\n",
      "Train loss:  0.123703055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9480069324090121\n",
      "Valid loss:  0.13085346\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Training epoch: 8"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9704219225750326\n",
      "Train loss:  0.0902901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9740034662045061\n",
      "Valid loss:  0.09340012\n",
      "Validation loss decreased (0.126319 --> 0.093400).  Saving model ...\n",
      "Training epoch: 9"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9717268377555459\n",
      "Train loss:  0.08924132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9618717504332756\n",
      "Valid loss:  0.087403916\n",
      "Validation loss decreased (0.093400 --> 0.087404).  Saving model ...\n",
      "Training epoch: 10"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9869508481948673\n",
      "Train loss:  0.056964323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9584055459272097\n",
      "Valid loss:  0.10018943\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Training epoch: 11"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9812962157459765\n",
      "Train loss:  0.061470542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9722703639514731\n",
      "Valid loss:  0.082184136\n",
      "Validation loss decreased (0.087404 --> 0.082184).  Saving model ...\n",
      "Training epoch: 12"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9882557633753806\n",
      "Train loss:  0.04073472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9584055459272097\n",
      "Valid loss:  0.09184895\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Training epoch: 13"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.983906046107003\n",
      "Train loss:  0.045876637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9722703639514731\n",
      "Valid loss:  0.083080366\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Training epoch: 14"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9856459330143541\n",
      "Train loss:  0.043119833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:06<00:00,  6.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9722703639514731\n",
      "Valid loss:  0.07579935\n",
      "Validation loss decreased (0.082184 --> 0.075799).  Saving model ...\n",
      "Training epoch: 15"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9926054806437582\n",
      "Train loss:  0.030076483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9722703639514731\n",
      "Valid loss:  0.08009463\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Training epoch: 16"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:26<00:00,  5.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9943453675511091\n",
      "Train loss:  0.027027337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9740034662045061\n",
      "Valid loss:  0.092498325\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Training epoch: 17"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 144/144 [00:28<00:00,  5.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy:  0.9917355371900827\n",
      "Train loss:  0.027494736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 37/37 [00:05<00:00,  6.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid accuracy:  0.9740034662045061\n",
      "Valid loss:  0.07700192\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Model early stopped\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, n_epochs + 1):\n",
    "    ###################   \n",
    "    # train the model #\n",
    "    ###################\n",
    "    \n",
    "    print(\"Training epoch:\", epoch, end='')\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    model.train()\n",
    "    \n",
    "    for img_lip, img_lip_m, label in tqdm(train_dataloader):\n",
    "        img_lip = img_lip.to(device)\n",
    "        img_lip_m = img_lip_m.to(device)\n",
    "        label = label.to(device) \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        clp_class = model(img_lip, img_lip_m)\n",
    "        loss = criterion_class(clp_class, label) \n",
    "        value, pred = torch.max(clp_class, 1)    \n",
    "        correct_tensor = np.sum(pred.eq(label.data.view_as(pred)).detach().to('cpu').numpy())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # save training losses \n",
    "        train_loss.append(loss.detach().to('cpu'))\n",
    "        train_acc.append(correct_tensor)\n",
    "    \n",
    "    train_acc = np.sum(np.array(train_acc))\n",
    "    print('Train accuracy: ', train_acc/len(train_df))    \n",
    "    train_loss = np.average(np.array(train_loss))\n",
    "    print('Train loss: ', train_loss) \n",
    "\n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    " \n",
    "    valid_loss = []\n",
    "    valid_acc = []\n",
    "    \n",
    "    model.eval() \n",
    "    for img_lip, img_lip_m, label in tqdm(val_dataloader):\n",
    "        \n",
    "        img_lip = img_lip.to(device)\n",
    "        img_lip_m = img_lip_m.to(device)\n",
    "        label = label.to(device) \n",
    "        \n",
    "        clp_class = model(img_lip, img_lip_m)\n",
    "        loss = criterion_class(clp_class, label) \n",
    "        value, pred = torch.max(clp_class, 1)    \n",
    "        correct_tensor = np.sum(pred.eq(label.data.view_as(pred)).detach().to('cpu').numpy())\n",
    "        \n",
    "        valid_loss.append(loss.detach().to('cpu'))\n",
    "        valid_acc.append(correct_tensor)\n",
    "    \n",
    "    valid_acc = np.sum(np.array(valid_acc))/len(val_df)\n",
    "    print('Valid accuracy: ', valid_acc)    \n",
    "    valid_loss = np.average(np.array(valid_loss))\n",
    "    print('Valid loss: ', valid_loss) \n",
    "\n",
    "    # Early_stopping checks if the validation loss has decresed\n",
    "    early_stopping(valid_loss, model)\n",
    "    # Save the best model\n",
    "    torch.save(model.state_dict(), dir_path)\n",
    "\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Model early stopped\")\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1e84f-88a7-4072-9e32-34c1a67a9119",
   "metadata": {},
   "source": [
    "#### Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2204e350-736d-4a76-9488-1270fbc20e30",
   "metadata": {},
   "source": [
    "##### Evaluation on test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ef9b287-39b3-40df-b0e8-1c0368543708",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:05<00:00,  6.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy:  0.9530332681017613\n",
      "Test loss:  0.13503034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(dir_path)) \n",
    "model.eval()\n",
    "\n",
    "test_acc = []\n",
    "test_loss = []\n",
    "\n",
    "for img_lip, img_lip_m, label in tqdm(test_dataloader):\n",
    "\n",
    "    img_lip = img_lip.to(device)\n",
    "    img_lip_m = img_lip_m.to(device)\n",
    "    label = label.to(device) \n",
    "    \n",
    "    clp_class = model(img_lip, img_lip_m)\n",
    "        \n",
    "    loss = criterion_class(clp_class, label) \n",
    "    value, pred = torch.max(clp_class, 1)    \n",
    "    correct_tensor = np.sum(pred.eq(label.data.view_as(pred)).detach().to('cpu').numpy())\n",
    "\n",
    "    test_acc.append(correct_tensor)\n",
    "    test_loss.append(loss.detach().to('cpu'))\n",
    "    \n",
    "test_acc = np.sum(np.array(test_acc))/len(test_df)\n",
    "print('Test accuracy: ', test_acc)    \n",
    "test_loss = np.average(np.array(test_loss))\n",
    "print('Test loss: ', test_loss) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22c7c83-bc96-498a-b079-bb4fc789cde8",
   "metadata": {},
   "source": [
    "##### Evaluation on test children set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e97669cd-eae1-4ea7-bc34-7388c6ee9b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00,  6.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test children accuracy:  0.8985507246376812\n",
      "Test children loss:  0.28554603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(dir_path)) \n",
    "model.eval()\n",
    "\n",
    "test_ch_acc = []\n",
    "test_ch_loss = []\n",
    "\n",
    "for img_lip, img_lip_m, label in tqdm(test_child_dataloader):\n",
    "\n",
    "    img_lip = img_lip.to(device)\n",
    "    img_lip_m = img_lip_m.to(device)\n",
    "    label = label.to(device) \n",
    "    \n",
    "    clp_class = model(img_lip, img_lip_m)\n",
    "        \n",
    "    loss = criterion_class(clp_class, label) \n",
    "    value, pred = torch.max(clp_class, 1)    \n",
    "    correct_tensor = np.sum(pred.eq(label.data.view_as(pred)).detach().to('cpu').numpy())\n",
    " \n",
    "    test_ch_acc.append(correct_tensor)\n",
    "    test_ch_loss.append(loss.detach().to('cpu'))\n",
    "    \n",
    "test_ch_acc = np.sum(np.array(test_ch_acc))/len(test_Ccr)\n",
    "print('Test children accuracy: ', test_ch_acc)    \n",
    "test_ch_loss = np.average(np.array(test_ch_loss))\n",
    "print('Test children loss: ', test_ch_loss) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310e4b04-a04a-4aa0-9c65-21dbacd90115",
   "metadata": {},
   "source": [
    "##### Evaluation on test adult set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4b8bc10-6560-495e-8b10-44bd0cc47299",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:01<00:00,  6.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test adult accuracy:  0.9354838709677419\n",
      "Test adult loss:  0.20528378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(dir_path)) \n",
    "model.eval()\n",
    "\n",
    "test_ad_acc = []\n",
    "test_ad_loss = []\n",
    "\n",
    "for img_lip, img_lip_m, label in tqdm(test_adult_dataloader):\n",
    "\n",
    "    img_lip = img_lip.to(device)\n",
    "    img_lip_m = img_lip_m.to(device)\n",
    "    label = label.to(device) \n",
    "    \n",
    "    clp_class = model(img_lip, img_lip_m)\n",
    "        \n",
    "    loss = criterion_class(clp_class, label) \n",
    "    value, pred = torch.max(clp_class, 1)    \n",
    "    correct_tensor = np.sum(pred.eq(label.data.view_as(pred)).detach().to('cpu').numpy())\n",
    " \n",
    "    test_ad_acc.append(correct_tensor)\n",
    "    test_ad_loss.append(loss.detach().to('cpu'))\n",
    "    \n",
    "test_ad_acc = np.sum(np.array(test_ad_acc))/len(test_cr)\n",
    "print('Test adult accuracy: ', test_ad_acc)    \n",
    "test_ad_loss = np.average(np.array(test_ad_loss))\n",
    "print('Test adult loss: ', test_ad_loss) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2703da-5eea-4d79-b527-d9bf0f720a83",
   "metadata": {},
   "source": [
    "##### Evaluation on CLP test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13d6e5f2-ecee-4d22-bf10-99ef8fbe08d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25/25 [00:04<00:00,  6.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test CL accuracy:  0.8759493670886076\n",
      "Test CL loss:  0.36773884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(dir_path)) \n",
    "model.eval()\n",
    "\n",
    "test_cl_acc = []\n",
    "test_cl_loss = []\n",
    "\n",
    "for img_lip, img_lip_m, label in tqdm(test_clp_dataloader):\n",
    "\n",
    "    img_lip = img_lip.to(device)\n",
    "    img_lip_m = img_lip_m.to(device)\n",
    "    label = label.to(device) \n",
    "    \n",
    "    clp_class = model(img_lip, img_lip_m)\n",
    "        \n",
    "    loss = criterion_class(clp_class, label) \n",
    "    value, pred = torch.max(clp_class, 1)    \n",
    "    correct_tensor = np.sum(pred.eq(label.data.view_as(pred)).detach().to('cpu').numpy())\n",
    " \n",
    "    test_cl_acc.append(correct_tensor)\n",
    "    test_cl_loss.append(loss.detach().to('cpu'))\n",
    "    \n",
    "test_cl_acc = np.sum(np.array(test_cl_acc))/len(test_clp)\n",
    "print('Test CL accuracy: ', test_cl_acc)    \n",
    "test_cl_loss = np.average(np.array(test_cl_loss))\n",
    "print('Test CL loss: ', test_cl_loss) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64a3a9b-7ca6-4434-9d8d-9789ba90aaf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "syncnet",
   "language": "python",
   "name": "syncnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
